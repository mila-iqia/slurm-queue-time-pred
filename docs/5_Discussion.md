# Discussion

<div align="justify">First, we find that the performance of the 6- and 7-layer models used to make predictions on Cedar and Graham cluster data are similar. Indeed, in both cases, around 40% of the predictions are lower than 2 times the target values and around 60% of them are lower than 3 times the target values. So if the job’s actual wait time is 15 minutes, the models predict a wait time in the interval [0, 30] minutes 40% of the time and in the interval [-15, 45] minutes 60% of the time. For jobs with short waiting times, therefore generally the smallest jobs, these prediction performances seem reasonable.
<br></br>
However, for a real wait time value of 10 hours, the models predict a wait time in the interval [0, 20] hours 40% of the time and in the interval [-10, 30] hours 60 % time. The difference between the prediction and the actual value is therefore much greater for jobs with a long waiting time (generally the largest jobs). In this case, the performances turn out to be less interesting and therefore cast doubt on the ability to deploy the models within SLURM.
<br></br>
One of the limitations of the project, which has an impact on the performance we obtained with our models, is the amount of data that was used for training. Part of this data had to be removed due to the reappearance of certain jobs over the days. A legitimate question to consider is the potential performance that could be achieved if a full year of data were available, as opposed to the current data set of around 60 days per compute cluster.
<br></br>
Nevertheless, the performance of our models appears to be much better than that of the SLURM estimator. Although a perfect comparison between the results of the models and those of the estimator is not possible due to the nature of the fictitious jobs (very small so as not to slow down the execution of other users’ jobs), one can all similarly get an idea of the order of the performance gap, which is around 10<sup>2</sup> or 10<sup>3</sup>.
<br></br>
Next, the reason we chose to do two separate experiments, the first using data from Cedar and the second, from Graham, is the particular configuration of SLURM on each of these clusters. Indeed, it is not guaranteed that a model which generalizes well on one cluster’s data has the same behavior on the other’s, and vice versa. This applies to any compute cluster. Some clusters, for example, have preemption (this is not the case for Cedar and Graham): the nature of the data taken from them would therefore vary a lot. A model developed on a cluster with preemption is unlikely to work on a cluster that does not have it, for example.
<br></br>
Finally, the performances of the training experiment on the past days are not as good compared to those of the experiment where the roles are assigned randomly. In particular, with the data collected on Cedar and for T (test day) = 68, the percentage of predictions under 2 and 3 times the actual waiting time is 5 and 6% smaller. The more obvious explanation is that the best source of data to predict what will happen on day N is what happens on day N-1 (or even N+1). The further back in time you go, the less useful it is in general. In the random experiment, we probably come across many situations where a test day is close to one or more training days. Thus, by putting the test day at the very end, the predictive power of the model is reduced.
</div>