# Discussion

<div align="justify">First, we find that the performances of the neural networks used to make predictions on Cedar and Graham are similar. Indeed, in both cases, around 40% of the predictions fall into the interval [0.50*t, 2.0*t] and around 60% fall into the interval [0.33*t, 3.0*t]. So if the jobâ€™s actual wait time is 15 minutes, the models predict a wait time in the interval [7.5, 30] minutes 40% of the time and in the interval [4.95, 45] minutes 60% of the time. For jobs with short waiting times, therefore generally the smallest jobs, these prediction performances seem reasonable.
<br></br>
The usefulness of these levels of confidence depends on the context. If we have access to two different compute clusters and we want to make an informed decision about sending jobs that have a wait time of 30 minutes on one cluster and 4 hours on the other, then those kinds of estimates are indeed useful. The odds of selecting the best of the two clusters for that task are good.
<br></br>
However, if the two clusters actually have respective wait times of approximately 5 hours and 10 hours, it's not very comforting to think that we have 40% chances for each of getting estimates in the interval of [2.5 hours, 10 hours] for the first and [5 hours, 20 hours] for the other. These estimates do not help us very much in assessing whether our job will run before lunch today, or tomorrow evening. That being said, the silver lining is that they might still indicate which choice is the best, in expectations.
<br></br>
One of the limitations of the project, which has an impact on the performance we obtained with our models, is the amount of data that was used for training. Part of this data had to be removed due to the reappearance of certain jobs over the days. A legitimate question to consider is the potential performance that could be achieved if a full year of data were available, as opposed to the current data set of around 60 days per compute cluster.
<br></br>
Nevertheless, the performance of our models appears to be much better than that of the SLURM estimator. More data should be collected if we really wanted to make a bullet-proof case about how bad the SLURM estimator is, but from our small-scale experiment in <a href="2_Results.md">Results</a>, they are off by a factor of at least 100.
<br></br>
We have data coming from two clusters, Cedar and Graham, but we have kept all the experiments and models separate. We felt that those two clusters, which have different hardware configurations and often different users, would better be served by having two separate models. As it is challenging enough for a model to generalize well from training data to testing data within a given cluster, we were pessimistic about training a single model for both. However, that is something that could be done and maybe we would realize that they are more similar than we think. We can say for sure, however, that the behaviors that we have seen on SLURM clusters with pre-emption differs greatly from SLURM clusters without pre-emption.
<br></br>
Finally, it is worth commenting about how the performance achieved in section <a href="2_Results.md">Results</a> is slightly (6%) better than that in <a href="3_Train_on_past_data.md">Train_on_past_data</a>. It is simply easier to make predictions about day N based on data from days N-1 or N+1, than if we were to look at data that was farther away in time. When we assign days at random to be in the test set, they tend to be neighbors with days from the training or validation sets. This is not the case when the test set contains only the very last day. In the context of a system in production where we make predictions for jobs being queued today (trained with past data), we should probably have some kind of importance weight so that days in the distant past are not counted as much. However, things could get complicated because data collected in the rush period leading up to a deadline exactly a year ago might actually be more useful in predicting the outcomes, compared with data collected a few months ago when everything was calm.
</div>
